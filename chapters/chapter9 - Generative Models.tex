So far we have explored \textbf{Supervised Learning}, the goal of which is, given a set of data \((\textbf{X}, y)\), to learn a function that maps to \(\textbf{X} \longrightarrow y\). For example, regression or image captioning belong to this category of machine learning tasks.

Another fundamental class of tasks is \textbf{Unsupervised Learning}, which, given only the data \(\textbf{X}\), the algorithm learns some underlying hidden structure. For example, tasks such as clustering and density estimation belong to this class.

In general, models that belong to unsupervised learning can be divided into two categories: \textbf{non-probabilistic} and \textbf{probabilistic generative}. The latter will be the subject of this chapter. The goal of this class of models is to solve the so-called \textbf{density estimation} task.

Given a set of training data with a certain \texttt{distribution of training data} \(\sim p_{\text{\textbf{data}}}(x)\), the model aims to approximate this underlying distribution in order to generate new samples. To do this, the model learns a \texttt{distribution of generated samples} \(\sim p_{\text{\textbf{model}}}(x)\), such that \(p_{\text{\textbf{model}}}(x)\) is as close as possible to \(p_{\text{\textbf{data}}}(x)\).

We can further categorize the generative models according to whether the density of the samples has been explicitly specified or not, namely \textbf{Explicit Density Estimation Models} (e.g., \(p_{\text{\textbf{model}}}(x)\) should be a normal distribution whose parameters are unknown) and \textbf{Implicit Density Estimation Models}. These differ because the model can sample from \(p_{\text{\textbf{model}}}(x)\) without explicitly defining the density function. An example of Explicit Density Estimation are VAEs, which explicitly define and solve for \(p_{\text{\textbf{model}}}(x)\). An example of Implicit Density Estimation are GANs, which learn a model that can sample from \(p_{\text{\textbf{model}}}(x)\) without explicitly defining it.

\section{Generative Adversarial Networks (GANs)}

The Generative Adversarial Networks, first introduced in the article \href{https://arxiv.org/pdf/1406.2661}{"Generative Adversarial Nets" (Goodfellow et al.)}, are probabilistic generative models that do not rely on explicit density functions. Instead, GANs exploit an \textbf{adversarial approach with two neural networks}: the \textbf{Generator} and the \textbf{Discriminator}.

The central role is that of the Generator: it learns to transform random noise from a simple distribution (such as random noise) into \textbf{complex samples that resemble those from a target distribution} (e.g., real images). This transformation process is supervised by the Discriminator, which simultaneously \textbf{learns to distinguish between real data samples and those generated by the Generator} (i.e., "fake" samples).

The adversarial aspect of this setup is crucial, where the Generator aims to produce samples indistinguishable from the real data in order to fool the Discriminator, while the latter tries to accurately differentiate between real and fake samples. This adversarial interaction drives both networks to improve iteratively, as each performance improvement in one network requires a corresponding adjustment in the other.

\newpage
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/chapter9 - Generative Adversarial Network.pdf}
    \caption{GAN Architecture}
\end{figure}

As you can see, the Generator takes random noise \(z\) from a simple distribution \(p_z\) and transforms it into fake samples \( x_{\text{fake}} \) that resemble real data. The Discriminator receives both real samples \( x_{\text{real}} \) from the true data distribution \( p_{\text{\textbf{data}}}(x) \) and the fake samples from the Generator. It then classifies them as real or fake, \( D(x) \). The adversarial training process drives the Generator to produce increasingly realistic samples to fool the Discriminator, which in turn becomes better at distinguishing real from fake.

\subsection{Objective Function of GANs}

The training procedure involves two "players", the Generator and the Discriminator, who are jointly trained through a \textbf{min-max optimization problem}:

\vspace{-0.8cm}
{\Large

\begin{equation*}
min_{\theta_{g}} max_{\theta_{d}}\left[E_{x \sim p_{data}}  \log (\textcolor{mygreen}{D_{\tikzmarkk{Y}\theta_{d}}(x)}) + E_{z \sim p_{z}}\log (\textcolor{myred}{1-D_{\tikzmarkk{YC}\theta_{d}}(G_{\theta_{g}}(z))})  \right]
\end{equation*}
\begin{tikzpicture}[overlay,remember picture]
    \node (Ye) [below of = Y, node distance = 3.8 em, anchor=west] {\footnotesize \textsf{\textcolor{mygreen}{Discriminator Output for Real Data}}};
    \draw[<-, in=180, out=-90] (Y.south)++(.25em,-1ex) to (Ye.west);

    \node (YCe) [below of = YC, node distance = 2 em, anchor=west]     {\parbox{\widthof{Discriminator Output for}}{
    \ \\
    \footnotesize \textsf{\textcolor{myred}{Discriminator Output for}} \\ 
    \footnotesize \textsf{\textcolor{myred}{Generated Fake Data $G(z)$}}}};
    \draw[<-, in=180, out=-90] (YC.south)++(.25em,-.5ex) to (YCe.west);
\end{tikzpicture}
}

\vspace{1.4cm}
The Discriminator outputs a probability in the range (0,1) for real data. Each network has its parameters: \(\theta_{d}\) for the Discriminator and \(\theta_{g}\) for the Generator. Each module is described by a function, \(\mathbf{D_{\theta_{d}}(x)}\) and \(\mathbf{G_{\theta_{g}}(z)}\). 

The Discriminator (\(\theta_{d}\)) aims to maximize the objective such that \(D(x)\) is close to 1 (real) and \(D(G(z))\) is close to 0 (fake). Conversely, the Generator (\(\theta_{g}\)) aims to minimize the objective such that \(D(G(z))\) is close to 1 (tricking the Discriminator into believing \(G(z)\) is real).


\subsection{Training GANs}
During training, we use Stochastic Gradient Descent alternating between \textbf{gradient ascent} on the discriminator and \textbf{gradient descent} on the generator, respectively:

\vspace{-0.5cm}
{\Large
\begin{equation*}
\begin{aligned}
    &\max_{\theta_{d}} \left[E_{x \sim p_{data}} \log (D_{\theta_{d}}(x)) + E_{z \sim p_{z}} \log (1 - D_{\theta_{d}}(G_{\theta_{g}}(z))) \right] \\
    &\min_{\theta_{g}} \left[E_{z \sim p_{z}} \log (1 - D_{\theta_{d}}(G_{\theta_{g}}(z))) \right]
\end{aligned}
\end{equation*}
}

The function to be optimized is complex, so the training is \textbf{unstable}. In addition, it is very difficult to monitor the GAN process because the objective function is not related to the quality of the generated samples.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{tikz/chapter9 - SGD GAN Plot 1.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO (only the BLUE one)}} SGD Plot for Generator during Training}
\end{figure}

In the figure, the line represents the gradient for the generator and the left part is when the generated sample is probably false. In this area, the gradient is quite \textbf{flat} and corresponds to the region where the model should learn more to improve its generating ability. In contrast, in the right part, the gradient signal dominates, which means that the sample is already good and not useful for learning purposes. Conceptually, \textbf{we waste a lot of resources in our area of interest, making learning more difficult}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{tikz/chapter9 - SGD GAN Plot 1.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO (only the GREEN one)}} SGD Plot (Inverted) for Generator during Training}
\end{figure}

Therefore, one way to improve training is to modify this part of the objective function and turn it into a \textbf{maximization problem}. This means that instead of minimizing the probability that the discriminator is correct, the model now \textbf{maximizes the probability that the discriminator is wrong}. As shown by the curve above, now the flat area has been inverted in the region outside our interest. Thus, the new training procedure requires alternating between two \textbf{ascending gradient} procedures:

\vspace{-0.5cm}
{\Large
\begin{equation*}
\begin{aligned}
    &\max_{\theta_{d}} \left[E_{x \sim p_{data}} \log (D_{\theta_{d}}(x)) + E_{z \sim p_{z}} \log (1 - D_{\theta_{d}}(G_{\theta_{g}}(z))) \right] \\
    &\ \textcolor{mybluee}{{\max_{\theta_{g}} \left[E_{z \sim p_{z}} \log (D_{\theta_{d}}(G_{\theta_{g}}(z))) \right]}}
\end{aligned}
\end{equation*}
}

Although the training efficiency has been improved, there are still some stability problems.

\subsection{Inference using GANs}
In probabilistic generative models, the inference phase is called the \textbf{sampling phase}: after training, the Discriminator is discarded and the Generator is used to produce samples similar to the training data from random noise.

Conceptually, the \textbf{Discriminator is just an auxiliary object} to match the distribution learned from the Generator with the distribution of our data.

The results showed that if this version of GAN is trained on an \textbf{unconstrained dataset}, i.e., one with a very wide distribution of labels (MNIST is a simple dataset since the digits have a similar distribution, while CIFAR-10 is a more complex one), it \textbf{performs poorly} regardless of the models used for both the Generator and the Discriminator (such as FC or CNN networks). However, the advantages of using the Generator network are that it can be used as a backbone for CNN or in NLP.

In the following sections, we will analyze the main problems of GANs and see some articles that have tried to mitigate them. These problems can be listed as follows:
\begin{itemize}
    \item \textbf{Overfitting and Control}: We do not have effective tools to determine whether the model is overfitting. The lack of clear indicators makes it difficult to implement techniques to control and prevent overfitting.
    \item \textbf{Evaluation of GANs}: There are no standardized and reliable metrics to evaluate the performance of a GAN. The quality of the samples generated can be subjective and difficult to quantify.
    \item \textbf{Training stability}: The training of GANs is notoriously unstable. 
    \item \textbf{Parameter oscillations and divergence}: Model parameters may oscillate or diverge during training. 
\end{itemize}

Another serious problem is the \textbf{Mode collapse}. This phenomenon occurs when the Generator, instead of capturing all modes of the target distribution, ends up capturing only one sub-mode, i.e., it learns only a certain area of the target distribution.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{tikz/chapter9 - Mode Collapse MNIST.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Mode Collapse Example}
\end{figure}

The figure shows an example of mode collapse with the MNIST dataset: instead of learning to generate all the numbers, the Generator focuses only on one area of the overall distribution. As a result, after many iterations, it generates only number one.

The following figure, on the other hand, shows the heatmap of the generator's distribution after an increasing number of training steps. It can be seen that the model never converges to a fixed distribution, but always assigns a significant probability mass to only one data mode at a time.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/chapter9 - Mode Collapse HeatMap.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Mode Collapse HeatMap}
\end{figure}


\section{DCGANs}

Through the work of \href{https://arxiv.org/pdf/1511.06434}{"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks" (Radford et al.)}, the DCGAN model was introduced. This represents the first GAN architecture based on deep convolutional networks and was the first model to generate high-resolution images using GAN. In fact, multiple convolutional layers are used to produce high-quality images (64x64).

The architecture of the discriminator differs slightly from a classical CNN in that it \textbf{does not use pooling}, but only convolutions with stride. In principle, pooling is effective in models such as AlexNet because it helps avoid overfitting, but in GANs it negatively affects the discriminator's ability to judge whether an image is real or fake, making discrimination too easy.

In addition, Leaky ReLU is used as the activation function, and only one fully connected layer is present before the softmax output. For normalization, the model employs batch normalization after most levels.

Batch normalization is beneficial because it speeds up training, however, applied to DCGAN could lead to reduced performance because it \textbf{introduces correlation between samples within the minibatch}. The effect of correlation leads to a \textbf{loss of the diversity property} (samples not different enough) during the generation of samples by the Generator. As a result, the model will not be able to generalize.

\subsection{Arithmetic of GAN}

In the same paper, the authors explored the latent space of GANs trained on a dataset of celebrity faces. 

They found that, through training, \textbf{the generator learns to map points in the latent space} (which by itself has no intrinsic meaning) \textbf{to specific images}, and this mapping will be different each time the model is trained.

Also, the article showed that, to compute the output image, the generator performs \textbf{vector arithmetic with faces}. For example, the figure below shows that a smiling woman's face minus a neutral woman's face plus a neutral man's face results in a smiling man's face.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{tikz/chapter9 - Arithmetics of GAN.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Example of Vector Arithmetic in GANs.}
\end{figure}

In this case, arithmetic is performed on the $z$ vectors (points in latent space) corresponding to each resulting face. The results showed that the Generator has some interpolation capabilities (interpolation means that the vectors in latent space can be combined to obtain a sample whose features are a combination of the two).

\section{Evaluation of GANs}

Evaluating a GAN model is complex because of the lack of clear methods for assessing the visual quality of the generated images. The first attempt was the use of human feedback, which naturally involves problems of subjectivity and monetary costs.

Moreover, \textbf{we are not only interested in the quality of the generated images, but also in the overall distribution of the GAN-generated samples}, which should be similar to our dataset. However, there is no direct way to calculate the probability of high-dimensional samples (real or generated) or to compare their distributions.

Thanks to \href{https://arxiv.org/pdf/1511.01844}{"A note on the evaluation of generative models" (Theis et al.)}, we will explore two widely used metrics: \textbf{Inception Score} and \textbf{Fréchet Inception Distance (FID)}.

To simplify the evaluation problem, we can divide into two main properties: the ability of the GAN to generate \textbf{recognizable objects} and the \textbf{variety of objects} generated.
\begin{itemize}
    \item If the GAN produces concentrated distributions when it generates images belonging to the same class, it means that each of these images is associated with a high probability (\textbf{recognizable objects}) of belonging to that certain class.
    \item Globally, the dataset generated by GAN should be able to produce a \textbf{variety of objects}.
\end{itemize}

Both properties are associated with probability (likelihood). This is computed using a classifier, such as Inception-v3 (state of the art at the time). Then, we use its output (probability distribution) to calculate the inception score.

\begin{remark}{mygreen}{mygreen!20}
The inception score is calculated based on the output of an image classification model and is maximized if the entropy of the distribution of labels for the generated images is minimized (\textbf{recognizable}) and the predictions of the classification model are uniformly distributed over all possible labels (\textbf{variety}).
\end{remark}

However, this metric has an overfitting problem: with this method, a GAN that simply stores training data could still score well. If this statement does not make sense to you, try thinking, "\textit{Why would a GAN that generates samples that are very similar to my training set be useful?}" Obviously, such a model is not useful since it simply copies the images seen. 

Therefore, a better approach is to use the Fréchet Inception Distance (FID).

\begin{remark}{mybluee}{mybluee!20}
Using FID, the generated samples are passed through a classification network and for a chosen layer the activations are calculated. The metric then takes the embeddings of that layer and treats them as \textbf{multivariate normal distributions}. The mean and covariance are calculated for both the generated samples and the actual data and compared using Multivariate Normal Fréchet Distance. However, \textbf{the overfitting problem is not completely avoided}.
\end{remark}

\section{Different GAN Losses}

The loss function plays a key role in GANs: \textbf{different losses correspond to different ways of comparing the two distributions}.

Let us now analyze two different loss functions compared with the standard GAN: \textbf{LSGAN} and \textbf{Wasserstein GAN}.

LSGAN was introduced in \href{https://arxiv.org/pdf/1611.04076}{"Least Squares Generative Adversarial Networks" (Mao et al.)} with the goal of stabilizing the training of a GAN.

Cross entropy is replaced by \textbf{Least Square Loss}:

\vspace{-0.5cm}
{\Large
\begin{equation*}
\begin{aligned}
    &\min_{\theta_{d}} \left[\frac{1}{2}E_{x \sim p_{data}} \left(D_{\theta_{d}}(x) - 1\right)^2 + \frac{1}{2}E_{z \sim p_{z}} \left(D_{\theta_{d}}(G_{\theta_{g}}(z))\right)^2\right] \\
    &\min_{\theta_{g}} \left[\frac{1}{2}E_{z \sim p_{z}} \left(D_{\theta_{d}}(G_{\theta_{g}}(z)) - 1\right)^2\right]
\end{aligned}
\end{equation*}
}

This leads to better quality generated images (better IS and FID) and more robust generators to mode collapse:

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/chapter9 - LSGAN.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Results Comparison between LSGAN and regular GAN}
\end{figure}

On the other hand, the work \href{https://arxiv.org/pdf/1701.07875}{"Wasserstein GAN" (Arjovsky et al.)} introduced the Wasserstein GAN with the idea of modifying the loss using a new method to better align the distributions of the dataset and the generated samples.

First, the \textbf{Wasserstein distance} is a metric that aligns two distributions with the concept of \textbf{optimal transport}. It measures the minimum cost of mass transport to convert the data distribution $q$ into the data distribution $p$.

Wasserstein GAN implements the idea of aligning the two distributions with this distance, providing \textbf{better gradients and more stable training}.

The most important result was that by implementing the Wasserstein GAN with the DCGAN architecture, it is possible to \textbf{correlate sample quality with the value of the Wasserstein distance}. This represents a huge improvement over other GANs. For example, not even DCGAN with the original divergence was able to achieve this property.

However, the more complex the distance, the more difficult the training becomes.

\section{GANs Training Tricks}

In this section, we explore the tricks suggested in this paper: \href{https://arxiv.org/pdf/1606.03498}{"Improved Techniques for Training GANs" (Goodfellow et al.)}.


\begin{outline}
\textbf{Feature Matching} aims to stabilize GAN training by modifying the generator's objective. Instead of trying to directly deceive the discriminator, the generator focuses on \textbf{matching the statistics of features} from an intermediate layer of the discriminator. By minimizing the difference between these feature statistics for real and generated data, the generator avoids overfitting to the discriminator's current decision boundary, resulting in more stable training and higher-quality samples.
\\

\textbf{Mini-batch Discrimination} helps the generator produce more diverse outputs by allowing the discriminator to \textbf{examine relationships between samples in a mini-batch}. An additional layer in the discriminator computes differences between pairs of samples, providing information on how each sample compares to others in the batch. This enables the discriminator to identify if the generator is creating similar or identical samples, thus encouraging the generator to generate a wider variety of outputs and avoiding the problem of mode collapse.
\\

The \textbf{Virtual Batch Normalization (VBN)} is a normalization method that extends traditional batch normalization. Regular batch normalization makes the output of a neural network for an input example highly dependent on other inputs in the same minibatch. To avoid this problem, in VBN each example is \textbf{normalized based on statistics collected on a reference batch of examples} chosen once and fixed at the beginning of training, and on itself. The reference batch is normalized using only its own statistics. 
\end{outline}


\section{GANs Zoo}
Since their advent in 2014, GANs have gone through numerous iterations and improvements. In this section, we will embark on a journey through the "GAN Zoo", exploring how an initially simple concept has evolved into a multitude of innovative variations. 

\subsection{Conditional GAN}

Introduced in \href{https://arxiv.org/pdf/1411.1784}{"Conditional Generative Adversarial Nets" (Mirza et al.)},a Conditional GAN (cGAN) \textbf{links an input with its corresponding label}, enabling the use of supervised information. For example, a photo of a lion paired with the label "\textit{lion}" can be used to guide the generation process. \textbf{The optimization problem remains unchanged}, so the techniques and knowledge applied to standard GANs are still relevant.

In the cGAN framework, the generator incorporates the \textbf{label as an extension of the latent space} \( z \). This can be achieved by appending a one-hot encoded label vector to \( z \). The discriminator also utilizes the label information of real data, enhancing its ability to differentiate between real and generated samples. With this additional information, the discriminator can focus on a specific class, improving its performance.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/chapter9 - Conditional GAN.pdf}
    \caption{Conditional GAN Architecture}
\end{figure}


\subsection{Image-to-Image Translation}

The paper \href{https://arxiv.org/pdf/1611.07004}{"Image-to-Image Translation with Conditional Adversarial Networks" (Zhu et al.)} introduces a conditional GAN architecture for image-to-image translation, known as \textbf{PIX2PIX Network}. The generative network transforms an input image into an output image, while the discriminative network evaluates how realistic and consistent the generated image looks with the input image. This approach is applicable to various tasks, such as translating photos into sketches, coloring black-and-white images, and improving image resolution. It is important to note that \textbf{training this model requires coupled/paired data}.


\subsection{Text-to-Image Synthesis}

Another interesting article is \href{https://arxiv.org/pdf/1605.05396}{"Generative Adversarial Text to Image Synthesis" (Reed et al.)}, which explores the use of GANs for image synthesis from textual descriptions. The generative network receives as input a textual description and a noise vector, and produces an \textbf{image that attempts to respect the content described in the text}. The discriminative network, on the other hand, distinguishes between real images with matching descriptions and generated images. This approach finds application, for example, in generating verbally described scenes or creating artwork from descriptions.

\subsection{Cycle GAN}



In the study \href{https://arxiv.org/pdf/1703.10593}{"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks" (Zhu et al.)} an innovative methodology is introduced for translating images between two different domains, \textbf{overcoming the need for matching pairs of images}. For example, it is possible to transform photographic images into artworks in the style of Monet using CycleGAN, even when the two collections are not paired.

The CycleGAN model is based on \textbf{three main components}: the Discriminator, Generator and Translator networks, as follows:

\begin{itemize}
\item \textbf{Translator}: this network is responsible for "converting" an image from one domain (e.g., photographic images) to another domain (such as Monet paintings).

\item \textbf{Discriminator}: trained to determine whether an image belongs to the target domain. It also verifies that the translated image, when converted back to the original domain, actually looks similar to the source image.

\item \textbf{Generator}: this network takes care of transforming the translated image back to the original domain, trying to preserve fidelity to the initial version.
\end{itemize}


The CycleGAN architecture modifies the classical GAN training process by adding a \textbf{Cycle-Consistency Loss (similar to L1 loss)} to ensure that the image returned after translation and reconstruction is as close as possible to the original image.

\textbf{The model operates in both directions}: it translates an image from one domain to another and then brings it back. This operation is replicated by reversing the roles of the domains and sharing parameters between the two directions.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{tikz/chapter9 - CycleGAN.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Cycle GAN Architecture}
\end{figure}

The figure represents the architecture of CycleGAN. On the top, the generative networks 
$G$ and $F$ handle the translation of images between the $X$ and $Y$ domains. The discriminators $D_X$ and $D_Y$ evaluate the generated images to confirm whether they belong to the respective domains. On the bottom we have the "unrolled" versions of the model and the concept of Cycle-Consistency Loss is illustrated, which ensures that an image translated between the two domains and then reconverted retains a similarity to the original image.

\subsection{Progressive GAN}

\href{https://arxiv.org/pdf/1710.10196}{"Progressive Growing of GANs for Improved Quality, Stability, and Variation" (Laine et al.)} introduces the Progressive GAN model, which introduces a novel approach for generating high-quality images by a process of \textbf{progressive growth of the Generator and Discriminator networks}. This method involves the gradual expansion of the networks during training, thus enabling high-quality images.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{tikz/chapter9 - Progressive GAN.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Progressive GAN architecture}
\end{figure}

The principle behind this approach is that, at each stage of training, both networks benefit from the parameters learned in the previous stages. Initially, the networks work with smaller image sizes. As the training continues and stabilizes, new blocks of layers capable of handling larger images are added. This gradual growth process allows the model to \textbf{first learn the main structures and then refine the details}, thus improving the final quality of the generated images.


\subsection{Style GAN}

The model created by NVIDIA and described in the paper \href{https://arxiv.org/pdf/1812.04948}{"A Style-Based Generator Architecture for Generative Adversarial Networks" (Laine et al.)} is named StyleGAN and brings significant innovations to the architecture of GANs through the use of a Generator that is able to capture and manipulate in detail the \textbf{style attributes of images}, such as pose, hair type, or smile shape. It also allows these attributes to be modified stochastically, using random noise to vary the styles.

In the StyleGAN model, fundamental changes were made to the Generator architecture, such as the introduction of a \textbf{style vector}. This vector, derived from a latent representation via 8 fully connected layers (FC), captures the stylistic features of the image.

StyleGAN uses \textbf{Instance Normalization} to manage the stylistic attributes of images. This process calculates, for each image, the mean (\(\mu\)) and standard deviation (\(\sigma\)) of the pixel values. Normalizing the image with these statistics \textbf{removes the variations in brightness and color that are specific to each image, without altering the structure of the content.} The statistics \(\mu\) and \(\sigma\) then capture stylistic aspects such as texture and color. Instance Normalization is applied at different layers of the network to extract and control various aspects of style. These stylistic attributes are then reintegrated at the final generation stage, allowing precise control over the stylistic details of the final image.

\subsection{Drag GAN (2023)}

\textit{GANs are back with an exciting new feature!} \emoji{yum} DragGAN represents a recent advancement in GAN models, allowing interactive editing of images via a simple drag-and-drop method, as described in \href{https://vcai.mpi-inf.mpg.de/projects/DragGAN/}{"Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold" (Pan et al.)}.

In the DragGAN model, the user defines pairs of \textbf{control points} and \textbf{target points}, which indicate the areas of the image to be modified. The goal is to drag portions of the image from the position of the control points to the target points.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{tikz/chapter9 - DragGAN.pdf}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO: Redo image using images of the paper}} DragGAN Example}
\end{figure}

The optimization process is divided into two main steps:
\begin{enumerate}
    \item \textbf{Motion Supervision}: A module that guides the control points to the desired target points.
    \item \textbf{Point Tracking}: A module that continuously updates the position of control points to reflect changes.
\end{enumerate}

DragGAN demonstrates that, despite the emergence of more advanced models, GANs continue to innovate, offering practical and interactive solutions for image generation.


\section{Autoencoders}

Autoencoders are unsupervised models that learn to represent input data in a space of reduced size and then reconstruct it. This process occurs in two main steps: the encoder and the decoder.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/chapter9 - Autoencoder.pdf}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-REFINE}} Autoencoder Architecture}
\end{figure}

The input is passed through the \textbf{Encoder}, a neural network that compiles data into a latent $z$ representation of lower dimension. This representation contains only the \textbf{most relevant features} extracted from the input. Next, the \textbf{Decoder}, which is a symmetric network with respect to the encoder, decodes the latent representation $z$ back to the size of the input to generate a reconstructed sample $x'$. The generated sample is then compared with the original input $x$ via a \textbf{reconstruction loss function (Norm L2)}.

The goal of training is to optimize the encoder and decoder weights to minimize this loss function. In particular, the encoder must refine its ability to extract essential features from the data, while the decoder must improve the quality of reconstruction of compressed features. After training, \textbf{the decoder part is generally discarded and the encoder can be used to initialize a supervised model} for predicting labels from the $z$ feature space.

Keeping the bottleneck ($z$) small forces the autoencoder to learn an intelligent representation of the data. Another approach to force the autoencoder to improve its representation is to add random noise to the inputs and have the model retrieve the original noise-free data, as described in the paper \href{https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf}{"Extracting and Composing Robust Features with Denoising Autoencoders" (Bengio et al.)}, which introduces the \textbf{Denoising Autoencoders}.

In many modern autoencoder architectures, the encoder is implemented as a CNN, to compress the data into a more compact representation. The decoder, which has a similar but opposite structure to the encoder, uses \textbf{Transposed Convolution Layers} to reconstruct the original image from this compact representation. Transposed convolution layers work in the opposite way to the convolution layers used in the encoder: \textbf{while the encoder reduces the size of the image, the decoder increases it}. This is useful for recreating the image from the reduced representation.

\section{Variational Autoencoders (VAEs)}

Traditional autoencoders are designed to \textbf{replicate input data from the same dataset}. Variational autoencoders enhance this capability by learning distributions for latent features, allowing them to generate \textbf{unique images} with \textbf{similar features} to those in the training dataset. VAEs create a \textbf{continuous latent space} that facilitates sampling and interpolation, generating new samples that are similar to the inputs but different from those in the training set.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/chapter9 - Variational Autoencoder.pdf}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-REFINE}} Variational Autoencoders Architecture}
\end{figure}

VAEs belong to the family of \textbf{explicit density estimation} models. Unlike models that indirectly measure the distribution of the data, VAEs use a direct approach to estimate the likelihood function of the data. The likelihood function for a sample of the dataset $x$ can be expressed as:

$$p_{\theta}(x)=\int_{} p_{\theta}(z)p_{\theta}(x|z)dz$$

where $z$ corresponds to the latent representation vector of our training data. 

The formula represents a likelihood function, thus we would like to learn the parameters $\theta$ that maximize it, i.e. such that generated data fit training data.

This formula represents a likelihood function, and the goal is to \textbf{learn the $\theta$ parameters that maximize it}, that is, those that best fit the generated data to the training data. However, we cannot compute this likelihood directly because it is mediated by $z$, which is a \textbf{latent representation learned from the training samples and not from the data itself}. Consequently, we do not have direct access to the data on which we are integrating.


To explain this problem, let us consider the formula in detail. It involves two probability distributions: $p_{\theta}(z)$ (\textbf{prior distribution}) represents the latent space, while $p_{\theta}(x|z)$ represents the Decoder, which samples $z$ from the prior distribution $p_{\theta}(z)$ and transforms it into a sample $x$ that must be similar to the training data.

Since the Decoder samples from the prior distribution, the latent space must be a simple probability distribution to facilitate this process. Therefore, the latent space is generally a \textbf{Gaussian Multivariate}. However, we cannot directly produce $p_{\theta}(z)$ since it is derived from the training data. \textbf{This is the integration problem}.

\textit{Here is a possible solution!} We could obtain $p_{\theta}(z)$ using Bayes' theorem. However, another problem would arise when we try to calculate the a posteriori probability distribution $p_{\theta}(z|x)$, since the denominator is intractable (\emoji{smiling-face-with-tear}):

$$p_{\theta}(z|x)= \frac{p_{\theta}(x|z)p_{\theta}(z)}{p(x)}$$

In this context, $p_{\theta}(z|x)$ represents the \textbf{latent space distribution extracted from the given input}.

\textbf{The solution to this problem is the Encoder}, which is trained to map the input image to the latent space, $q_{\phi}(z|x)$. In this way, the encoder approximates the a posteriori distribution $p_{\theta}(z|x)$. \textbf{\textcolor{myred}{Beware that the encoder does not provide a global solution to the optimization problem, only an approximation: it provides a lower bound on the likelihood of the data!}}

Since we are modeling probabilistic data generation, both the encoder and the decoder are probabilistic models and are described by their own parameters, $\phi$ and $\theta$, respectively. The Encoder learns the mean and variance associated with the Gaussian distribution of the latent space $z$. The Decoder learns the mean and variance of the generating distribution, which should be as similar as possible to the dataset data.  \textbf{Then, the Variational Autoencoder can be seen as an Autoencoder with a probabilistic component}.

The objective function can be derived as follows by applying log-likelihood and several transformations:

\begin{equation*} 
\begin{aligned}
\log (p_{\theta}(x)) &= E_{z \sim q_{\phi}(z|x)}\left[ \log p_{\theta}(x) \right]\\
&=E_{z }\left[ \log \frac{p_{\theta}(x|z) p_{\theta}(z)}{p_{\theta}(z|x)} \right]  \text{  (Bayes Rule)} \\
&=E_{z }\left[ \log \frac{p_{\theta}(x|z) p_{\theta}(z)}{p_{\theta}(z|x)} \frac{q_{\phi}(z|x) }{q_{\phi}(z|x)}\right] \text{(Multiply by Constant)} \\
&=E_{z }\left[ \log p_{\theta}(x|z) \right] - E_{z} \left[ \log \frac{q_{\phi}(z|x)}{ p_{\theta}(z)}\right] + E_{z} \left[ \log \frac{q_{\phi}(z|x)}{p_{\theta}(z|x)} \right]  \text{(Logarithms)} \\
&=\textcolor{mybluee}{E_{z }\left[ \log p_{\theta}(x|z) \right] - D_{KL} \left[ q_{\phi}(z|x)|| p_{\theta}(z)\right]}\textcolor{mygreen}{\ + \ D_{KL} \left[ q_{\phi}(z|x)||p_{\theta}(z|x) \right]} 
\end{aligned}
\end{equation*}

From the equation above, \textbf{\textcolor{mygreen}{the third term is excluded}} since we cannot compute it, since we do not have access to $p_{\theta}(z|x)$ (remember, the encoder \textbf{approximates} the a posteriori distribution). However, this term is always a positive value since it is calculated via the KL divergence, which allows us to approximate the likelihood as a lower bound. If you do not know, \textbf{KL divergence is a measure of how similar two probability distributions are}; if they are identical, the divergence is zero, while a positive number indicates that the distributions are different.

\textbf{\textcolor{mybluee}{Two terms remain}}: the first is the Reconstruction Loss or \textbf{\textcolor{mybluee}{Reconstruction Term}}. This term is applied to the decoder to train it to \textbf{generate meaningful results} when it samples from the latent representation relative to the input data.

The second term can be computed in closed form because it involves the KL divergence between two Gaussian distributions and is used for the encoder (since it tries to approximate the prior distribution, which is a Gaussian). This is often called \textbf{\textcolor{mybluee}{Regularization Term}} because it \textbf{forces the latent space to fit a Gaussian distribution}.

\textbf{The combination of these two factors allows the latent space to be continuous and well-structured}, ensuring smooth interpolation between latent representations without gaps. This forces the encoder to produce a Gaussian distribution that approaches the prior.

\begin{figure}[!htbp]
    \begin{minipage}[t]{.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tikz/chapter9 - Latent Space VAEs 1.jpeg}
        \subcaption{Only Reconstruction Term}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tikz/chapter9 - Latent Space VAEs 2.jpeg}
        \subcaption{Only Regularization Term}
    \end{minipage}  
    \hfill
    \begin{minipage}[t]{.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{tikz/chapter9 - Latent Space VAEs 3.jpeg}
        \subcaption{Both}
    \end{minipage}  
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO (replace subcaption with tikz nodes)}} Latent Spaces Comparison}
\end{figure}

The figure compares three different latent spaces obtained by various autoencoder training strategies. 

The latent space on the left is the result of \textbf{exclusive use of reconstruction loss}, as in traditional autoencoders. In this case, no constraint is applied on the latent space distribution. As can be seen, the latent space has no well-defined structure and the latent points are not distributed according to a centered Gaussian. In other words, the latent data appear scattered and disorganized, with no clear structure. \textit{Indeed, autoencoders are not generative models by nature, since there are no constraints on the latent space to guarantee its structured behavior. If we train an autoencoder, take the latent space and add noise, the decoder will produce meaningless results. }

The second latent space, located in the center of the figure, is obtained by using \textbf{only the regularization term}. In this scenario, the model learns a narrower distribution for each sample type, trying to fit a Gaussian shape. However, as highlighted in the figure, although the distribution assumes a Gaussian shape, the latent points tend to overlap and mix. This makes it difficult to distinguish the different data sets, as the latent data are not well separated.

Finally, the latent space on the right represents the result of \textbf{joint optimization of both terms: the reconstruction loss and the regularization term}. This combination allows the latent space to fit the data well, being continuous and similar to a Gaussian distribution. As a result, the latent space presents a uniform and continuous structure, with the data distinct in well-defined clusters, exactly what we want to achieve! \emoji{smiling-face-with-sunglasses}


Now, \textbf{to train VAEs we want to use backpropagation, but in practice this is impossible} (\emoji{pensive}) because sampling from the latent distribution $z \sim q_{\phi}(z|x)$ is not differentiable. \textit{However, the \textit{\textbf{reparametrization trick}} comes to our rescue!} This trick is to sample from a standard normal distribution $\epsilon \sim \mathcal{N}(0, I)$ and transform it with $z = \mu(x) + \sigma(x) \cdot \epsilon$. So, \textbf{the sampling process becomes differentiable with respect to the model parameters}, facilitating the training of latent representations. \emoji{smiling-face-with-sunglasses} (again)


\section{VQ-VAEs}

Typically, in a classic VAE, the prior ($p_{\theta}(z)$) and posterior ($q_{\phi}(z|x)$) are assumed to be normally distributed with diagonal variance. In the proposed work on VQ-VAEs \href{https://arxiv.org/pdf/1906.00446}{"Generating Diverse High-Fidelity Images with VQ-VAE-2" (Razavi et al.)}, however, the authors use \textbf{discrete latent variables} (instead of a continuous normal distribution). The posterior and prior distributions are categorical, and \textbf{samples drawn from these distributions index a embedding table}. Then, the encoders model a \textbf{categorical distribution}, from which we sample to obtain integral values. These integral values are used to index an embedding dictionary, and the indexed values are then passed to the decoder.

For example, in images we may have categories such as "Dog" or "Bicycle," and it would not make sense to mix these categories. Discrete representations are also easier to handle, since each category has a unique value. In contrast, in a continuous latent space, it would be necessary to normalize the density function and understand the relationships between the various variables, which can be very complicated.
    

\section{GAN and VAE Comparison}

{\color{red}\colorbox{pink}{Tikz TO-DO: tabella finale nelle slides}}
\textbf{Comparison of GAN and VAE using the Ideal Properties of Generative Models}

\textbf{High-Quality Sampling}: GANs are capable of generating high-quality samples, meaning the samples they produce are often indistinguishable from real data.

\textbf{Coverage}: GANs often struggle with coverage, meaning they might generate samples that represent only a subset of the training data, rather than covering the entire distribution. VAEs generally aim to cover the full distribution, though their success in this area can vary.

\textbf{Well-Behaved Latent Space}: Both models have well-behaved latent spaces. In other words, each latent variable $z$ corresponds to a realistic data example $x$, and smooth changes in $z$ lead to smooth changes in $x$.

\textbf{Interpretable Latent Space}: Neither GANs nor VAEs excel at providing an interpretable latent space. Ideally, altering each dimension of $z$ should affect a specific, understandable feature of the data, but neither model achieves this effectively.

\textbf{Efficient Likelihood Computation}: VAEs, despite being probabilistic models, struggle with efficiently computing likelihoods. GANs do not use likelihoods, so this characteristic does not apply to them.



% TUTTO STO BLOCCO ANDREBBE PRIMA DI VQVAE, VEDERE SE INSERIRLO, FARE UNA APPENDIX OPPURE RIMUOVERLO DEL TUTTO.
\begin{comment}


\paragraph{Latent vector}

As in the standard approach, the latent vector $ z $ encapsulates essential features (attributes) of a given input. Each attribute is represented by a \textbf{latent variable}, corresponding to a real feature of the object and their values  are derived from the original sample $x$. These attributes, as implied by the term \textbf{latent}, cannot be directly measured. An example is the age attribute shown in Figure \ref{fig:VAE-example}.

\textbf{Qeustion n.1}: How are these latent variables' values computed by the model?


The partial answer lies in the fact that the architecture has changed, now employing a probabilistic approach (Figure \ref{fig:VAE-architecture}). Specifically, the architecture consists of two modules:  the \textbf{Probabilistic Encoder}, represented by $q_{\phi}(z|x)$, and the \textbf{Probabilistic Decoder}, represented by $p_{\phi}(x|z)$. 

\textbf{Answer to question n.1}: 

Instead of compressing the features, the encoder maps the input into the feature distribution function $q_{\phi}(z|x)$, allowing the values to be continuous. 


This allows the probabilistic decoder to sample attribute values from each latent variable's distribution, using the resulting vector to reconstruct a new image with the conditional probability $p(\hat{x}|z)$, where $\hat{x}$  is the new sample.



Furthermore, the encoder divides the features size of $x$ into two parts, the mean $\mu$ and the standard deviation $\sigma$. Subsequently, they are join toghether into $z$ representation, by a linear transformation that involves $\mu$,  multiplying $\sigma$ with a random sample sampled from a standard normal distribution $\sim N(0,1)$.



\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/variational autoencoder.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Enter Caption}
    \label{fig:VAE-architecture}
\end{figure}



\textbf{First summarized difference}:

Using a VAE, we \textbf{represent each latent attribute for a given input as a probability distribution}, unlike standard autoencoders. For example, given an image of a man (Figure \ref{fig:VAE-example}), a standard AE fixes the smile feature, while a VAE learns a probability distribution for the smile. During decoding, we randomly sample from this distribution for each latent variable, generating a vector used by the decoder to create a new unique image. \textbf{The features of the generated image will belong to the same distribution as the input, but will be different from the samples in our dataset}.

Hence, that is why  we aim to construct our encoder model as a probabilistc one. It must be able to output a \textbf{range of possible values} for each latent attributes from which we will randomly sample to feed into our decoder model.


A visual explanation is provided in Figure \ref{fig:VAE-prob}. If we sample green and orange features from the smile probability distribution, they are close to each other in the latent space. Consequently, when these latent vectors are fed into the decoder, the generated images will look similar to the original one because their vectors are similar, and vice versa.


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/VAE latent space.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Enter Caption}
    \label{fig:VAE-prob}
\end{figure}




Before diving into the mathematical part and defining the training Loss, we still miss a crucial aspect for easily understanding VAE, which is the goal they try to pursue!

\paragraph{Goal}

\underline{Let us first state it in probabilistic fashion}:

The goal of VAE is to find a distribution $q_{\phi}(z|x)$ (the encoder) of some latent variables $z$ (latent attributes) from which we can sample from $z \sim q_{\phi}(z|\underline{x}) $ to generate a new samples $x^{'} \sim p_{\theta}(x^{'} |z)$ (see figure \ref{fig:VAE-architecture} to better understanding to whom this probability functions refer to). 

We want to stress the keyword \textbf{new} because, when we feed the sampled latent vector $z$ (containing the sampled values of each latent variable)  into the probabilistic decoder, it generates a sample $x^{'}$ that is not present within our dataset but \textbf{still}  has the same distribution of the original sample x.

\underline{Now, let's explaint this concept in a not probabilistic way}:

A VAE ensures that encodings from a known probability distribution can be decoded into reasonable objects, \textbf{even if they are the same of the actual image}, as they share the same probability distribution.

Traditional autoencoders cannot perform this task since they learn the "probability distribution" of a \textbf{restricted domain}. For instance, an autoencoder trained on the MNIST dataset will output random noise when given an image from a different dataset because the decoder cannot recognize the different probability distribution of the latent representation.

VAEs, instead of producing single-value encodings, produce a probability distribution function for \textbf{each encoding} (see Figure \ref{fig:VAE-example}). The decoder then samples from these distributions to cluster the images.





\paragraph{Loss Function}

Welcome to the madness! In this subsection we will describe the loss object of VAE in a problem-solution flow manner. Why such approach? Because many problems are linked to the probabilistic style of the two modules. 

Let's start with first defining the loss function:

$$L(\theta, \phi) = E_z \left[log p_{\theta}(x^{(i)}|z) \right]- D_{KL}(q_{\phi}(z|x^{(i)})||p_{\theta}(z))$$

\begin{itemize}
    \item The first term represents an the expected value of a probability distribution $z$, but \textbf{1. How is z distributed?}
    \vspace{5 pt}

    \item The second term is the KL divergence between $q_{\phi}(\underline{z}|\underline{x})$ and $p_{\theta}(z)$.
\end{itemize}


$q_{\phi}(\underline{z}|\underline{x}) $ and $p_{\theta}(\underline{x}|\underline{z}) $ are learnt through backpropagation. The parameters  of both modules will be updated in terms of $\phi$ and $\theta$.

\textbf{2. WHAT is KL divergence and 3.WHY we need it?}

\textbf{Answer to question number 2}: The KL divergence is a measure of how similar two probability distributions are; if they are the same, the divergence is zero, and if it is a positive number, the two distributions are different. 

The loss function guides us to build a parameterized model that estimates a distribution by minimizing the KL divergence between the original and our parameterized distribution. Generally, we would compute this divergence between possible distribution families $Q$ and find the optimal distribution that minimizes the distance to the actual distribution. \textbf{As we want to make the decoder's sampling easier},  $q$ is assumed to be a normal distribution (for each attributes/ multivariate guassian distribution for the $z$ vector).



Still, \textbf{why on earth we would use or even think about using this method}?

See in figure \ref{fig:VAE-architecture}, the probabilistic encoder is defined by the posterior probability $q_{\phi}(z|x)$, which means, given a sample, the encoder should be able to generate the distributions of each latent variables. So, in theory  to compute this probability we would only need the usage of the \textbf{Bayesian theorem}:

$$p(z|x) = \frac{p(x|z)p(z)}{p(x)}$$

However, in practice, the bayesian rule is difficult to compute because the denominator $p(x)$ is difficult to obtain as it requires to marginalize the nominator over \textbf{all} possible values of $z$:

$$p(x)= \int_{z}^{} p(x|z) p(z) dz = \int_{z} p(x,z)dz$$

The integral is not available in closed form and it is intractable due to multiple integrals involved for latent variable vector $z$ (they can be of any size).





\textbf{Answer to question number 3}: Thus, the alternative is to approximate $p(z|x)$ by another distribution $Q(z|x)$ which is defined in such a way that it has tractable solution. We approximate $p(z|x)$ using $Q(z|x)$ where $Q(z|x)$ is a simple distribution such as Gaussian. Thus, we keep track of the goodness of this approximation by computing KL between $p(z|x)$ and $Q(z|x)$.  

Then, to "learn" the parameter that better approximate $q(z|x)$, we should find them by an optimization procure that tries minimizing the KL divergence. 

\textbf{4. But how we can convert an inference problem into an optimization one?}

\textbf{Answer to question number 4}: this is done through \textbf{Variational inference}. The main goal of this technique is to pose the inference problem as an optimization problem:



\begin{equation}
\begin{aligned}
D_{KL} (Q_{\phi}(z|x) || p_{\theta}(z|x)) &= \sum_{z}Q_{\phi}(z|x)\log\left(\frac{Q_{\phi}(z|x)}{p_{\theta}(z|x)}\right) \\
&= E_{z \sim Q_{\phi}(z|x)}\left[ \log\left(\frac{Q_{\phi}(z|x)}{p_{\theta}(z|x)}\right) \right] \\
&= E_{z \sim Q_{\phi}(z|x)}\left[ \log(Q_{\phi}(z|x)) - \log(p_{\theta}(z|x)) \right]
\end{aligned}
\end{equation}


$Q_{\phi}(z|x) $ is a simple gaussian and $p_{\theta}(z|x)$ is the probability function that we are trying to approximate. Then, let's substitute the bayesian theorem into $\log(p_{\theta}(z|x))$ :


\begin{equation}
\begin{aligned}
D_{KL} (Q_{\phi}(z|x) || p_{\theta}(z|x)) &= E_{z}\left[ \log(Q_{\phi}(z|x)) - \log(p_{\theta}(z|x)) \right] \\
&= E_{z}\left[ \log(Q_{\phi}(z|x)) - \log(p_{\theta}(x|z)) - \log(p_{\theta}(z)) + \log(p_{\theta}(x)) \right]
\end{aligned}
\end{equation}



\textbf{Answer to question number 1}: the latent variables are distributes as a gaussian distribution \textbf{$z = z \sim Q_{\phi}(z|x)$}. 

Since the integration is over $z$ and $p(x)$ does not depend on $z$, it can be moved out.


$$D_{KL}(Q_{\phi}(z|x) || p_{\theta}(z|x)) - log P_{\theta}(x) = E_{z}\left[ log(Q_{\phi}(z|x))-log(p_{\theta}(x|z))-log(p_{\theta}(z))  \right] $$

By rearranging the terms we obtain:

$$D_{KL}(Q_{\phi}(z|x) || p_{\theta}(z|x)) - log P_{\theta}(x) = E_{z}\left[ log(p_{\theta}(x|z)) \right] - \textcolor{blue}{E_z\left[log(p_{\theta}(x|z))-log(p_{\theta}(z))  \right]} $$

The \textcolor{blue}{second} expression is the $D_{KL}$, hence:

$$D_{KL}(Q_{\phi}(z|x) || p_{\theta}(z|x)) - log P_{\theta}(x) = \textcolor{myred}{E_{z}\left[ log(p_{\theta}(x|z)) \right]} - \textcolor{mygreen}{D_{KL}(Q_{\phi}(z|x) || p_{\theta}(z))}$$

We have finally re-obtained the VAE objection function, where the first term represents the \textcolor{myred}{reconstrunction likelihood} (as Standard AE) and the \textcolor{mygreen}{second term} ensures that our leaned distributions $Q$ is similar to the prior distribution $p$.

$$L(\theta, \phi) = E_{z \sim Q_{\phi}(z|x)}\left[ log(p_{\theta}(x|z)) \right] - D_{KL}(Q_{\phi}(z|x) || p_{\theta}(z))$$

So our target is to find optimal $\theta$ and $\phi$ such that the loss is minimized. VAE has an \textbf{inherit regularized ($D_{KL}$)}.  $\theta^{*}$ are the weights for the encoder network, $\phi^{*}$ are the weights of the generation network.

Lets explain the loss function:

\begin{itemize}
    \item $E_{z \sim Q_{\phi}(z|x)}\left[ log(p_{\theta}(x|z)) \right]$: it is the expectation carried out over the log probability function $log(p_{\theta}(x|z))$. Sample of $z$ are taken from $Q_{\phi}(z|x)$, which is the neural network represented by the \textbf{encoder model} that maps x to z . 

    \vspace{10 pt}
    
    \item The inner term is called \textbf{log-likelihood}. If $p_{\theta}(x|z)$ is assumed to be a multivariate (because we are working with a vector) gaussian distribution ($p_{\theta}(x|z) \sim N (\mu_{\theta}(z)), \sum_{\theta}(z) $) and take the log, we get a square error of the data sample x and mean of the guassian distribution, i.e the \textbf{L2 norm}. It determines \textbf{how far the reconstructed data \hat{x} is from the input x}. \textbf{Thus, this term refers to the encoder module $p_{\theta}(x|z)$ as it maps the latent vector into a generated sample \hat{x}}

    \vspace{10 pt}

    \item $D_{KL}(Q_{\phi}(z|x) || p_{\theta}(z))$: represents the divergence between $Q_{\phi}(z|x)$ and $p_{\theta}(z)$. Note that here we assume to know $p_{\theta}(z)$ which is a gaussian with mean 0 and variance 1. \textbf{So, this part referes to the encoder model $Q_{\phi}(z|x)$ as it maps x into z}. We want this data generated z to be not far from the normal distribution. It acts like a regularizer as it doesn't allow z to diverge from a normal distribution.

    \vspace{10 pt}
    
    \item Therefore, when sampling the z vector before being passed into the decoder module, its values will be a combination of z gaussian, form example $Q_{\phi}(z_1|x)$ and $Q_{\phi}(z_2|x)$. Consequently, the generated sample from z will not be part of the dataset but will still belong to the same distribution.
\end{itemize}

\textbf{Summary difference n.2}

The intepretation of the loss is: while reconstruction loss enables the distribution to correctly describe the input, by focusing only on minimizing the reconstruction loss, the network learns very narrow distributions. The KL divergence loss prevents the network from learning narrow distributions and tries to approximate posterior closed to the prior.


The goal of this derivation is to allows us to derive a lower bound on the data likelihood that is tractable and we can optimize. We have successfully casted the \textbf{inference problem into an optimization problem}!

The \textbf{lowebound} of $log p_{\theta}(x)$ is the loss $L(\theta, \phi) $ itselfe, as  KL divergence is always non negative, thus: 

$$L(\theta, \phi) \le log p_{\theta}(x)$$

Therefore when minimizing the loss, we are maximizing the lower bound of the probability of generating of data samples similar to the original data, \textbf{not identicall} but still from the same distribution.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/VAE posterior.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} The overall image describes the whole flow. In few words, we want to know how to learn how to generate data, x, from our latent variables, z. This implies that we want to learn $p(z|x)$. To learn it, we use the KL distance to approximate the true posterior probability that cannot be computed by bayesian theorem; \textbf{Right}: We can choose prior p(z) to be simple, e.g. Gaussian; Conditional p(x|z) is complex (generates image) and it is represented with Decored neural network; \textbf{Left}: The additonal probabilistic encoder.}
    \label{fig:VAE-posterior}
\end{figure}





\end{comment}