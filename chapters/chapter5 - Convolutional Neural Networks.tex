Welcome to the second part of the course, in which we delve into the intricate world of \textbf{different neural network architectures}. \textit{If things seemed complicated before, they will get even more challenging here.} \emoji{smiling-face-with-tear}

Convolutional Neural Networks (CNN) handles \textbf{structured data}, such as images. Various architectures have been developed, each attempting different methods to learn effective representations of structured data. In the case of images, these representations capture \textbf{spatially local dependencies} between pixels. The low-level layers of a CNN extract local features, while the high-level layers extract learn global
patterns.

\textit{To understand Convolutional Neural Networks, think of our brain. When we look at an image, the brain processes it in different parts. At first, it recognises simple things like lines and angles. Then, it moves on to more complicated things, until it understands the complete object.}

\textit{The cells in the brain are like small parts that only look at one part of the image. This helps to focus only on the important things. There are two main types of these cells: some look at small and close things, others look at larger things and care less about the exact position.}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{tikz/chapter5 - CNN.pdf}
    \caption{Example of a Convolutional Neural Network}
\end{figure}

Convolutional Neural Networks represent a \textbf{specialised version of feedforward neural networks}. If we were to apply a feedforward neural network to process an image, we would lose important information on the spatial relationships between pixels and would have to deal with a large number of parameters due to the need to treat the image as a long input vector. This situation therefore necessitates a complete redesign of the architecture.

CNNs are designed with some specific features, discussed in the previous chapter, that define their recurrent module. These features include \textbf{local connectivity} and \textbf{weight sharing}, which allow these architectures to effectively capture the spatial characteristics of images without having to manage an excessive number of parameters.

\section{Convolutions}

Convolutional Neural Networks basically \textbf{replace general matrix multiplication with convolution in at least one of their layers}. A convolution is an operation that takes a matrix (a portion of an image corresponding to the size of the so-called kernel) as input and produces a scalar by calculating the scalar product between the pixel values of the area and the kernel parameters. By progressively moving the kernel, the final output of the convolutional layer will be a matrix. 

Formally, a Convolutional Layer consists of a set of filters, each covering a small spatial portion of the input, known as the \textbf{receptive field}. Each filter is convoluted along the dimensions of the input data, producing a multidimensional feature map representing the filter's responses to various fragments of the input data. \textbf{During training, the network learns filters that are activated in response to specific features at specific spatial locations}.

This concept can be formalised as follows:
$$
S(i,j) = \sum_{m}\sum_{n} I(m,n)K(i-m,j-n)
$$
Convolutional layers help to extract local features from an image using filters, which are learnt during training as they store weights. However, in a CNN, \textbf{filters} are not learnt randomly; rather, they \textbf{are optimised to best fit the architecture, loss function and data}.


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{tikz/chapter5 - Convolution.pdf}
    \caption{Example of a 2D Convolution}
\end{figure}
An example of convolution is shown in the figure above, in which we have this kernel and calculations:

\hspace{0.05\textwidth}
\begin{minipage}{0.4\textwidth}
$$
\begin{pmatrix}
\mathbf{\textcolor{mygreen}{1}} & \mathbf{\textcolor{mygreen}{0}} & \mathbf{\textcolor{mygreen}{1}} \\[-0.2cm]
\mathbf{\textcolor{mygreen}{0}} & \mathbf{\textcolor{mygreen}{1}} & \mathbf{\textcolor{mygreen}{0}} \\[-0.2cm]
\mathbf{\textcolor{mygreen}{1}} & \mathbf{\textcolor{mygreen}{0}} & \mathbf{\textcolor{mygreen}{1}} \\
\end{pmatrix}
$$
\end{minipage}
% \hspace{0.05\textwidth}
\begin{minipage}{0.4\textwidth}
$$
\begin{aligned}
&\mathbf{\textcolor{myred}{1}} \times \mathbf{\textcolor{mygreen}{1}} + \mathbf{\textcolor{myred}{0}} \times \mathbf{\textcolor{mygreen}{0}} + \mathbf{\textcolor{myred}{0}} \times \mathbf{\textcolor{mygreen}{1}}&\\
+ &\mathbf{\textcolor{myred}{1}} \times \mathbf{\textcolor{mygreen}{0}} + \mathbf{\textcolor{myred}{1}} \times \mathbf{\textcolor{mygreen}{1}} + \mathbf{\textcolor{myred}{0}} \times \mathbf{\textcolor{mygreen}{0}}&\\
+ &\mathbf{\textcolor{myred}{1}} \times \mathbf{\textcolor{mygreen}{1}} + \mathbf{\textcolor{myred}{0}} \times \mathbf{\textcolor{mygreen}{0}} + \mathbf{\textcolor{myred}{1}} \times \mathbf{\textcolor{mygreen}{1}}&=\mathbf{\textcolor{mybluee}{4}}\\
\end{aligned}
$$
\end{minipage}
\hspace{0.05\textwidth}

For the input considered, the convolution is calculated through the scalar product between the kernel values and the corresponding input values. 
The result of this calculation is 4, which represents the output of this specific convolution.

It is important to know two fundamental parameters used to control the size of the convolution output and influence the size and shape of the features extracted by the convolutional neural network, namely:
\begin{itemize}
    \item \textbf{Padding}: Padding refers to the addition of values (commonly zeros) around the edges of the input image before applying the convolution. This is useful for controlling the size of the convolution output. Padding may be "\textbf{Valid}" (no padding) or "\textbf{Same}" (padding so that the output is the same size as the input, also called \textbf{zero-padding}).
    \item \textbf{Stride}: The stride indicates the number of pixels in which the filter (kernel) is moved along the input image during the convolution operation. \textbf{A stride value greater than one} results in a larger displacement, reducing the size of the output. \textbf{A stride value of one} is the most common and produces an output with the same size as the input.
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{tikz/chapter5 - Convolution with Padding.pdf}
    \caption{Example of a 2D Convolution with Zero-Padding}
\end{figure}

The output of the convolutional layer depends on the size of the kernel $K$, the stride $S$, the padding $P$, and can be calculated using the formula $\mathbf{\frac{(N-K)+2P}{S}+1}$, where $N$ is the size of the input. In the case of our example, the input is a square matrix of size 7x7, the kernel is 3x3, the padding is 1 and the stride is 1, so the output will be a matrix of size 5x5.

\textbf{N.B.} It is important to note that when we speaks of "two-dimensional convolutions", we are actually working with three-dimensional tensors (volumes). Thus, the example of the two-dimensional convolution given above does not fully reflect the complexity of convolutions in CNNs. In fact, when exploring various architectures, such as AlexNet or VGGNet, it is evident that the drawings of the various layers show the use of three-dimensional tensors. If you want to try and explore this further, here is a useful link that shows what these convolutions actually look like visually: \underline{\href{https://animatedai.github.io/}{Animated AI Convolution}}.


\section{Inside a CNN}

Here's an overview of what happens inside a Convolutional Neural Network:  
\begin{remark}{accent}{accent!5}
\textbf{Input image}: This layer represents the raw image entering the network. Generally, images are represented as multidimensional arrays of pixels, where each pixel has a value indicating its light intensity or color.
\end{remark}

\begin{remark}{mybluee}{mybluee!20}
\textbf{Convolution layer}: This is where the convolution operation takes place, which is fundamental for extracting features from the input image. Here, filters (or kernels) are applied to the image to detect specific patterns. \textbf{Each filter}, as it flows over the entire image, \textbf{produces a feature map}, highlighting the presence of edges, shapes and textures. \textit{It is as if each small region of the image participates in a "discussion" process to identify its unique features, and the different filters carry on this discussion, each highlighting different aspects.} The size of a filter is not constrained, allowing the network to adapt and learn different aspects of the images.
\end{remark}


\begin{remark}{myred}{myred!20}
\textbf{Non-linearity}: After convolution, a non-linear activation function such as ReLU is applied. This layer adds non-linearity to the model, allowing the network to learn and represent complex relationships between image features, without affecting the
receptive fields. 
\end{remark}

\begin{remark}{mygreen}{mygreen!20}
\textbf{Spatial Pooling}: This layer reduces the size of feature maps obtained by convolution, reducing the number of parameters and controlling overfitting. Commonly used are \textbf{max pooling} or \textbf{average pooling} layers, which reduce the size of the feature map by \textbf{preserving only the most significant values}. This also helps to create a \textbf{spatially invariant representation}, as relevant features are preserved even if slightly shifted in the image. \textit{So, the network is able to classify a photo of a dog even if it is translated in the image.}
\end{remark}

\begin{remark}{myorange}{myorange!20}
\textbf{Normalization Layer}: This layer is optional and can be used to normalise feature maps before passing them on to the next layer of the network. As we have seen, Normalization can be useful to ensure that \textbf{data remain in an appropriate range} and to facilitate the network learning process.
\end{remark}

\begin{remark}{accent}{accent!5}
\textbf{Feature Maps}: The feature maps resulting from the previous steps represent the information extracted from the input image. Each feature map corresponds to a specific feature detected by the network and can be interpreted as a \textbf{simplified representation of the image in terms of relevant patterns and structures}. These feature maps are then used as input to subsequent layers of the network for further processing and analysis.
\end{remark}
\textit{Nope, the colors are not related with the colors in the pictures above, sorry.}

\section{CNN Architectures (for Classification)}

Let's now discover the main architectures of Convolutional Neural Networks, from the pioneer LeNet to the modern ConvNeXt, which have revolutionised image recognition!

\subsection{LeNet (1998)}
LeNet was the \textbf{milestone that initiated the field of Convolutional Neural Networks}, developed at AT\&T Labs. Its impact has been instrumental in promoting the use of Convolutional Neural Networks in character recognition and other computer vision applications. Its main features include:
\begin{itemize}
    \item Uses \textbf{convolutional and subsampling} (pooling) followed by fully connected layers.
    \item \textbf{Requires complete retraining when image resolution changes}.
    \item Article: \href{http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf}{"Gradient-Based Learning Applied to Document Recognition" (LeCun et al.)}
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{tikz/chapter5 - LeNet.pdf}
    \caption{LeNet Architecture}
\end{figure}

\subsection{AlexNet (2012)}
AlexNet marked the \textbf{beginning of the deep learning revolution} and was the first modern CNN architecture. Its main features include:

\begin{itemize}
    \item It has a similar structure to LeNet, but with a \textbf{larger model} (60M parameters) and \textbf{more data} (thanks to ImageNet).
    \item Because the resolution of the input is higher than LeNet (224x224), more hidden layers were needed (7 in this case).
    \item It was developed to participate in the \textbf{ImageNet Challenge competition} and outperformed all the previous models in the challenge.
    \item It was the \textbf{first GPU implementation} of a neural network (2 in parallel, that's why each CN layer is splitted into two halfs).
    \item Also, AlexNet used for the first time ReLu activation function instead of Tanh (because obtaininng the same performance using a Relu requires less time than using tanh).
    \item \textbf{Universal Feature Extractor}: AlexNet showed that features learnt from convolutional layers (features before the output layer, 4096 dimensional features for each image) can be generalised and reused effectively for a wide range of computer vision tasks, making it a universal feature extractor (as they become quite discriminative). Also, a study showed that features used in computer vision before AlexNet were way less discriminative even than features extracted after the first convolutional layer of the network. Finally, the features extracted from AlexNet are so powerful and discriminative that, when applied to another dataset (such as SUN-397) or to a different task from the original one, they produce \textbf{equally effective results without needing to be redefined or re-trained}.
    \item Article: \href{https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}{"ImageNet Classification with Deep Convolutional Neural Networks" (Krizhevsky et al.)}
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{tikz/chapter5 - AlexNet.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} AlexNet Architecture}
\end{figure}


\subsection{ILSVRC - Zeiler and Fergus Net (2013)}
ILSVRC 2013 introduced similar architecture to AlexNet (5 convolutional layers and 2 fully connected layer at the end each with 4096 units), but with some significant innovations. Its main features include:
\begin{itemize}
    \item Use of \textbf{smaller kernels in the first convolutional layers} (from 11x11 to 7x7).
    \item Use different number of channels. Why? Because their goal was to see visually what's happen inside a CNN.
    \item Introduction of the \textbf{explicability of CNNs through deconvolutional networks}.
    \item Article: \href{https://arxiv.org/pdf/1311.2901.pdf}{"Visualising and Understanding Convolutional Networks" (Zeiler and Fergus)}
\end{itemize}

In order to better understand what happens within a convolutional neural network and to identify the areas of interest on which it focuses to classify an image, we use \textbf{Deconvolutional Networks}. The work of Zeiler and Fergus is fundamental in this field and has made it possible to visualise activity in the intermediate layers of a CNN.

A Deconvolutional Network aims to interpret the activations in the various layers of a CNN by mapping them in pixel space. It works by reversing the convolution process: it is run in reverse and used as a probe without the need for learning. To do this, you \textbf{connect a Deconv Net to a layer of the CNN} you wish to probe, pass the input image through the CNN to obtain the activations, and then map these activations to pixel space.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{tikz/chapter5 - Deconvolutional Networks.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Functioning of a Deconvolutional Network with Unpooling}
\end{figure}

The deconvolution process includes several steps: unpooling, rectification and filtering. Unpooling is performed using \textbf{switch variables to keep track of the positions of the maxima in the original pooling}, as max pooling is not invertible. Rectification uses the ReLU function to propagate only non-negative values, while filtering applies transposed versions of the learned filters.

The result is a visualisation that shows which features of the original image were relevant for classification, allowing us to understand which patterns the algorithm found discriminative for a given class and better understand the inner workings of the CNN to improve its performance and interpretability.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.47\textwidth]{tikz/chapter5 - Deconvolutional Networks Captured Patterns}
    \caption{Patterns Captured in a Layer of a CNN}
\end{figure}

Another method to visualise whether the CNN is learning correctly is to \textbf{occlude an image at various positions} and observe the feature activations and classifier confidence. 

Zeiler and Fergus' principle is based on developing a scheme to interpret the internal behaviour of neural networks, and occlusion experiments are a useful tool in this context.

There are also other ways of visualising the inner workings of a CNN, such as \textbf{Class Model Visualisation}, introduced in \href{https://arxiv.org/pdf/1312.6034}{"Deep Inside Convolutional Networks: Visualising
Image Classification Models and Saliency Maps" (Simonyan et al.)}, which is a more rigorous approach than the alternative. Essentially, a model can be viewed as a function that generates a vector containing the predictions for each class, so its size depends on the number of classes. One could consider reversing the optimisation process within a CNN: instead of adjusting the weights of the neural network through back-propagation, the goal is to find an \textbf{image that maximises the score for a specific label}. 

However, two main challenges arise: different optimisers may produce different solutions and different optimisers may introduce distinct numerical optimisation problems. This final point is related to the fact that this approach requires the implementation of regularisation techniques.

An important technique introduced later to better understand how CNNs work is \textbf{Grad-CAM} (Gradient-weighted Class Activation Mapping), which \textbf{evidences relevant} regions of an image for classification.
It was introduced in the article \href{https://arxiv.org/pdf/1610.02391}{"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization" (Selvaraju et al.)}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{tikz/chapter5 - GradCam.pdf}
    \caption{Grad-CAM applied to an Example Image}
\end{figure}

The intuition is that gradients of the classification score with respect to the final convolutional feature map are used to identify the parts of an input image that most influence the classification score. The places where this gradient is large are those where the final score is most dependent on the data.

To calculate the Grad-CAM, we proceed as follows:

\begin{itemize}
    \item The gradient of the score for the class \(c\) before softmax with respect to the feature maps of a convolutional level is calculated.
    \item A global pooling is performed on the obtained gradients.
\end{itemize}

\subsection{VGGNet (2014)}
VGGNet represents an extension of the ideas introduced by AlexNet, with increased depth. It was the last NN developed with the AlexNet-like structure: alternation of convolutional layers and pooling layer until a small dimension is reached, and then attach a fully connected layer.

Its main features include:
\begin{itemize}
    \item Very deep architecture with \textbf{more than twice the parameters of AlexNet}.
    \item Keeps the sequence of convolutions, subsamples and layers fully connected.
    \item One of the first architectures to demonstrate that \textbf{network depth is a critical factor for performance}.
    \item Article: \href{https://arxiv.org/pdf/1409.1556.pdf}{"Very Deep Convolutional Networks for Large-Scale Image Recognition" (Simonyan and Zisserman)}
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{tikz/chapter5 - VGGNet.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} VGGNet Architecture}
\end{figure}

\subsection{GoogleNet (2014)}
The primary issue with VGG was the exponential increase in the number of parameters. Subsequently, neural network architectures underwent a dramatic shift towards significantly reducing the number of parameters. This reduction was prompted by the recognition that the alternating architecture employed by AlexNet had become impractical. Instead of expanding the parameter count, the focus shifted towards increasing the number of layers (depth). A prominent example of this "new NN way" is GoogleNet, A 22-layer CNN but with a reduction in the number of parameters: from 60
million (AlexNet) to 4 million.

GoogleNet introduced an innovative structure, reducing the number of parameters compared to AlexNet. Its main features include:
\begin{itemize}
    \item Does not use fully connected layers, \textbf{reducing the complexity of the model} (inception module) and the risk of overfitting.
    \item Usage of supervision both at the end and intermidiate layers in order to fight with vanishing gradient (a deeper network is more prone to). 
    \item Usage of \textbf{inception modules}, allowing the network to capture details at different scales. An inception module consists of multiple convolutional layers with different kernel sizes, which are concatenated to capture features of different scales simultaneously. This parallelism helps GoogleNet capture fine-grained details as well as high-level features, making it exceptionally effective at image recognition tasks.
    \item Use of \textbf{1x1 convolutions to reduce the number of channels}, improving computational efficiency and reducing the computational workload.
    \item Introduction of \textbf{batch normalization}, which helps stabilise and speed up the training process. This is because if the inception block is trained with batch normalization, performance improves, even achieving the same performance as if not using batch normalization, but in fewer steps.
    \item Article: \href{https://arxiv.org/pdf/1409.4842.pdf}{"Going Deeper with Convolutions" (Szegedy et al.)}
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{tikz/chapter5 - GoogleNet.png}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} GoogleNet Architecture}
\end{figure}


Furthermore, the same authors of this paper suggested some guidelines for designing deeper
architectures:

\begin{enumerate}
    \item \textbf{Avoid extreme compression}: When designing a deeper CNN, it is important not to over-compress the information from the previous layer. It is preferable to gradually reduce the size of the representation rather than doing so drastically.

    \item \textbf{Higher-dimensional representations are easier to process locally}: It means that within a network, it is easier to process representations that maintain a larger spatial dimension rather than compressing them too much.

    \item \textbf{Reduce the input size before spatial aggregation}: Using 1x1 filters to reduce the input size before combining spatial features can be done without significantly losing the representation.

    \item \textbf{Balancing the depth and width of Inception modules}: It is not yet completely clear how to best distribute the computation between height, width, and depth within an Inception module. However, it is advisable to look for an optimal balance to achieve good results.
\end{enumerate}

\subsection{ResNet (2015)}

Generally, if we have a NN with 18 layers and another with 34 layer, we would think that the last should perform better. Unfortunately, it is not the case and performance degradation is seen. ResNet addressed the problem of \textbf{performance degradation} of deep neural networks by introducing the idea of identity layers, also known as skip connection or identity bypass. Its main features include:
\begin{itemize}
    \item \textbf{Identity Layers}: allow the model to learn to ignore certain layers when they are not needed.
    \item Using \textbf{many small kernels}, reducing the overall number of parameters and improving computational efficiency.
    \item ResNet can be seen as the \textbf{implicit ensemble of more superficial neural networks}, thanks to skip connections. In fact, if we unrolled a connection encompassing a residual blocks, we would have multiple paths through the network.
    \item Skip connections make the \textbf{loss landscape much more uniform} and easier to optimise during the training process.
    \item Article: \href{https://arxiv.org/pdf/1512.03385.pdf}{"Deep Residual Learning for Image Recognition" (He et al.)}
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{tikz/chapter5 - ResNet.pdf}
    \caption{ResNet Architecture}
\end{figure}

After ResNet, CNN designs did not used anymore final fully connected layers if the model is deeper enough. Also, batch normalization became the only method used as regularization method for CNNs, allowing removal/decrease of dropout and L2 regularization.

There are two remarkable architectures inspired by ResNet: 

\textbf{DenseNet} is an evolution of ResNet where each layer is directly connected to all other layers in the block. This promotes the reuse of features extracted from previous layers, improving gradient propagation and training stability. Advantages:
\begin{itemize}
    \item \textbf{Feature reuse}: Each layer receives input from all previous layers in the dense block, enabling efficient reuse of information.
    \item \textbf{Parameter reduction}: As each layer uses existing features, the need to learn new features from scratch is reduced, improving the overall efficiency of the network.
\end{itemize}

\textbf{SENet} (Squeeze-and-Excitation Network) improves ResNet by introducing Squeeze-and-Excitation blocks that adaptively recalibrate features based on channel importance. This is done by calculating a weight for each channel using a sigmoidal activation function. Advantages:
\begin{itemize}
    \item \textbf{Feature recalibration}: SE blocks compute weights for each channel, focusing on the most relevant ones, thus improving the network's ability to adapt to crucial details and patterns.
    \item \textbf{Performance improvement}: SENet has been shown to improve the accuracy of neural networks while reducing the number of parameters needed.
\end{itemize}

\subsection{ConvNeXt (2022)}
ConvNeXt represents a CNN architecture that has proven to be \textbf{competitive with modern transformers}. Its main features include:
\begin{itemize}
    \item Designed to compete with modern transformer models.
    \item Use of \textbf{connection structures between layers} that enable more effective learning of spatial and semantic relationships.
    \item Article: \href{https://arxiv.org/pdf/2201.03545.pdf}{"A ConvNet for the 2020s" (Liu et al.)}
\end{itemize}


\section{Regularisation Techniques for CNNs}
Convolutional Neural Networks have demonstrated excellent learning capability on a wide range of computer vision tasks. However, like many other machine learning techniques, CNNs are \textbf{susceptible to the phenomenon of overfitting}, in which the model overfits the training data, reducing its ability to generalise to new data.

To mitigate overfitting and improve the performance of CNNs, several specific regularisation techniques have been developed. In this section, we will examine some of these popular techniques: Cutout, Dropblock, Cutmix.

\begin{outline}
Cutout is a regularisation technique that consists of \textbf{cutting a random rectangular area of the input image} (16x16) during the training process. This cut region is subsequently filled with the average pixel value of the entire dataset. \textit{In practice, we force the model to consider more parts of the image during training, thus reducing the risk of overfitting.} \\

Dropblock is an extension of the dropout technique, in which instead of randomly deactivating certain neurons during training, \textbf{entire regions of neurons within a convolutional layer are deactivated}. This prevents the network from focusing too much on specific input features, promoting a more robust and general feature representation. Other variations are Dropout, drops individual values, and SpatialDropout, drops a whole channel.\\

Cutmix: During training, \textbf{a random rectangular area of an image is replaced with a corresponding area from another image}, while class labels are mixed in proportion to the replaced area. This technique encourages the model to learn useful features from multiple regions of the image, while providing a form of regularisation.
\end{outline}


\section{Object Detection}

Image classification is only one of the tasks that Convolutional Neural Networks can tackle. In addition to this, there are more complex tasks such as object detection, in which it is necessary not only to classify the objects in the image, but also to \textbf{identify their exact location through bounding boxes}. In this section, we will explore the strategies used to deal with this challenge using \textit{our beloved} CNNs.

Object detection is a complex task as it requires \textbf{both the classification of objects and the prediction of their positions through bounding boxes}. Thus, it implies doing simultaneously a classification and a regression task (for predicting continuous values, i.e. the bounding box). The input we provide is still an image, but the outputs are bounding boxes with labels for each object in the image. 

This approach makes the training of networks more difficult than simple classification, as there are problems such as data sparsity and difficulty in accurately locating objects. 

Before the advent of models such as AlexNet, the situation of object detection was at a standstill, with competition results stable on a plateau. However, the introduction of innovative techniques revolutionised this field.

\subsection{Selective Search for Object Recognition (2013)}
Selective Search is an image segmentation algorithm used to generate \textbf{proposed regions of interest} (RoI) for object recognition. The idea is to produce many "patches" in the image, and for each the algorithm assigns a likelihood score of that region containing the object of interest: regions with low likelihood are very unlikely to cointain it (and viceversa).

Its main features include:

\begin{itemize}
    \item \textbf{Groups similar pixels together to form regions} and combines them into larger segments.
    \item \textbf{Preliminary Step}: identifies possible areas of the image that might contain objects before proceeding with classification.
    \item Using a hierarchical approach, the algorithm groups pixels according to specific criteria to form a "tree", which is used to obtain bounding boxes.
    \item Article: \href{https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf}{"Selective Search for Object Recognition" (J.R.R. Uijlings et al.)}
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{tikz/chapter5 - Selective Search.png}
    \caption{Training Procedure for Selective Search}
\end{figure}


\subsection{R-CNN - Region-based Convolutional Neural Network (2014)}
R-CNN is the \textbf{first convolutional neural network-based approaches for object recognition}. Its main features include:

\begin{itemize}
    \item \textit{It's nothing but Selective Search + AlexNet}.
    \item Use \textbf{proposed regions} to identify objects in the image.
    \item Uses a convolutional neural network to \textbf{extract features and classify objects within these regions}: We begin by taking an image and employing selective search to extract a set of region proposals ($\sim$2k/image). For each proposal, we fine-tune AlexNet individually, extracting features and training a classifier to determine whether the patch contains the target object.
    \item \textbf{Efficiency problem}: R-CNN uses a multi-stage model and Selective Search to explore the entire space of bounding boxes. Also, Selective Search is a "fixed" algorithm that does not dynamically adapt to the image, making it necessary to resize the input before passing it through the neural network, resulting in a \textbf{naive pipeline} approach. Lastly, both training and testing are very slow; for example, testing requires to rerun selective search, about 2k forward pass per images. Despite this, R-CNN performed significantly better than shallow models.
    \item Article: \href{https://arxiv.org/pdf/1311.2524.pdf}{"Rich feature hierarchies for accurate object detection and semantic segmentation" (Ross Girshick et al.)}
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{tikz/chapter5 - R-CNN.png}
    \caption{R-CNN Pipeline}
\end{figure}

\subsection{Fast R-CNN (2015)}
Fast R-CNN is an improved version of R-CNN that \textbf{addresses the efficiency issues} of its predecessor. Its main features include:

\begin{itemize}
    \item \textbf{Region of Interest (RoI) pooling}: Introduces a RoI pooling layer that allows the best use of the information extracted from the convolutional network for each proposed region. This level makes it possible to \textbf{reduce the size of the proposed regions as input} to the final classifier without loss of significant information.
    \item \textbf{Convolution of the forward pass of a CNN}: Fast R-CNN shares the forward pass of a CNN on the image across its selected sub-regions. This means that \textbf{convolution is performed only once} for the entire image, reducing the computational load and improving the overall efficiency of the model.
    \item Article: \href{https://arxiv.org/pdf/1504.08083.pdf}{"Fast R-CNN" (Ross Girshick)}
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{tikz/chapter5 - Fast R-CNN.jpeg}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Fast R-CNN Architecture. In the figure, the backbone block is responsible to extract every feature from the image, thus allowing to perform the forward pass of CNN only one time per image; RoI pooling extracts regions of proposal and are reshaped them into a fixed size. This helps to feed the features of each sub-region into a (shallow) fully connected layer,  by which we can perform classification and regression tasks.   }
\end{figure}


\subsection{Faster R-CNN (2015)}
Faster R-CNN is another significant development in the field of object detection. Differently to R-CNN and Fast R-CNN which use selective search to find out the region proposals, Faster R-CNN eliminates it as it is a slow and time-consuming process. Instead,  this model lets another the network to learn the region proposals. Its main features include:

\begin{itemize}
    \item \textbf{Similarity with Fast R-CNN}: Like Fast R-CNN, the image is provided as input to a CNN that provides a convolutional feature map.
    \item \textbf{Use of a separate network for proposal generation}: Instead of using Selective Search, a separate network called RPN (Region Proposal Network) is used in Faster R-CNN to predict region proposals. The predicted region proposals are then reshaped using a RoI pooling layer which is then used to classify the image within the proposed region and predict the offset values for the bounding boxes.
    \item \textbf{End-to-end architecture}: With the integration of the RPN module, Faster R-CNN allows the entire end-to-end model to be trained in a single step, improving efficiency and ease of training.
    \item Article: \href{https://arxiv.org/pdf/1506.01497.pdf}{"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks" (Shaoqing Ren et al.)}
\end{itemize}


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.65\textwidth]{tikz/chapter5 - Faster R-CNN.jpeg}
    \caption{{\color{red}\colorbox{pink}{Tikz TO-DO}} Faster R-CNN Architecture}
\end{figure}



\subsection{YOLO - You Only Look Once (2016)}
YOLO is an innovative approach to object detection that \textbf{stands out for its speed and efficiency}. Its main features include:

\begin{itemize}
    \item YOLO treats the problem of object detection as a \textbf{direct regression problem}, in which the entire image is divided into a grid and for each grid cell the bounding box and class probability of the objects within that cell are directly predicted.
    \item \textbf{Use of a single convolutional network}: Unlike previous methods that use regions to locate objects in the image, YOLO uses a single convolutional network to predict bounding boxes and class probabilities for these boxes. This makes YOLO's approach simpler and more efficient.
    \item \textbf{Split image into grid}: YOLO splits the image into an SxS grid and for each grid cell predicts a set of bounding boxes. This approach allows YOLO to consider several regions of the image simultaneously.
    \item \textbf{Bounding box selection}: For each predicted bounding box, the YOLO grid produces a class probability and offset values for the bounding box. Bounding boxes with a class probability above a threshold are selected and used to locate the object in the image.
    \item Article: \href{https://arxiv.org/pdf/1506.02640.pdf}{"You Only Look Once: Unified, Real-Time Object Detection" (Joseph Redmon et al.)}
\end{itemize}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{tikz/chapter5 - YOLO.png}
    \caption{YOLO Architecture}
\end{figure}

\section{Instance Segmentation}

In addition to object classification and bounding box detection, there is another challenge in computer vision: instance segmentation. It consists of \textbf{assigning a label to each pixel of an image}, distinguishing individual objects within the scene. This is an \textbf{even more complex challenge} than simple object detection, as it requires a detailed understanding of the structure and shape of the objects themselves.

One of the most advanced techniques to address this challenge is known as Mask R-CNN, which extends the Faster R-CNN framework for instance segmentation. 

\subsection{Mask R-CNN (2017)}

Mask R-CNN is an extension of Faster R-CNN that adds an \textbf{additional branch to the neural network for instance segmentation}. Its main features include:

\begin{itemize}
    \item Mask R-CNN uses \textbf{RoI Align}, an improved version of RoI Pooling, to ensure more precise alignment of feature maps during the instance segmentation phase. This significantly improves the quality of segmentation masks.
    \item \textbf{Pixel-level segmentation}: Mask R-CNN extends Faster R-CNN for pixel-level segmentation, allowing a label to be assigned to each pixel in the image based on the object instance to which it belongs.
    \item \textbf{FCN added}: Mask R-CNN adds a Fully Convolutional Network on top of selected features to produce accurate and detailed segmentation masks.
    \item Article: \href{https://arxiv.org/pdf/1703.06870.pdf}{"Mask R-CNN" (Kaiming He et al.)}
\end{itemize}

